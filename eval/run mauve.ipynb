{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Author: Krishna Pillutla\n",
    "# License: GPLv3\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import auc as compute_area_under_curve\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm.auto import tqdm as tqdm_original\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:37:51.902339500Z",
     "start_time": "2024-05-05T09:37:49.637562800Z"
    }
   },
   "id": "8df897a8dc16c775"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'/tmp/ibEIwVjwsP'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:47:02.010011700Z",
     "start_time": "2024-05-05T09:47:01.997499500Z"
    }
   },
   "id": "b82c6cefad9940f3",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e00c4c09f53467d988ed059ea71e318"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e4c11b413fd4e55bc8fa0ce5d4ae8f0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c71d673c55624441b584d94856d14b2e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90243ef94a284bfb9037da83f4a879da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72601c414a7b4391823a910f6712c886"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20cb4ea1c5ac4de593aed88bd5ae64ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3014f2c8c5b40aab46e80610e68aa2f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8300e32be7b24cf3923de0146ac1602f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d436b236b424801b8a3e84d65fa1bda"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    import torch\n",
    "\n",
    "    FOUND_TORCH = True\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    FOUND_TORCH = False\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "\n",
    "    FOUND_TRANSFORMERS = True\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    FOUND_TRANSFORMERS = False\n",
    "\n",
    "HF_TOKEN = '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'\n",
    "\n",
    "MODEL, TOKENIZER, MODEL_NAME = None, None, None\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "MODEL: LlamaForCausalLM = AutoModelForCausalLM.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "MODEL.half()\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:41:19.717721900Z",
     "start_time": "2024-05-05T09:37:51.945330800Z"
    }
   },
   "id": "52311f0ef3196378"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"gpt2\"\n",
    "# MODEL = AutoModel.from_pretrained(MODEL_NAME)\n",
    "# TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:41:19.718722200Z",
     "start_time": "2024-05-05T09:41:19.717721900Z"
    }
   },
   "id": "51f8d70aa9bea5ae"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "CPU_DEVICE = torch.device('cpu')\n",
    "tqdm = lambda *args, **kwargs: tqdm_original(\n",
    "    *args,\n",
    "    **kwargs,\n",
    "    disable=os.environ.get(\"DISABLE_TQDM\", False)\n",
    ")\n",
    "\n",
    "\n",
    "def get_device_from_arg(device_id):\n",
    "    if (device_id is not None and\n",
    "            torch.cuda.is_available() and\n",
    "            0 <= device_id < torch.cuda.device_count()):\n",
    "        return torch.device(f'cuda:{device_id}')\n",
    "    else:\n",
    "        return CPU_DEVICE\n",
    "\n",
    "\n",
    "def get_model(model_name, tokenizer, device_id):\n",
    "    print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')\n",
    "    device = get_device_from_arg(device_id)\n",
    "    if 'gpt2' in model_name or \"bert\" in model_name:\n",
    "        model = AutoModel.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id).to(device)\n",
    "        model = model.eval()\n",
    "    else:\n",
    "        raise ValueError(f'Unknown model: {model_name}')\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_tokenizer(model_name='gpt2'):\n",
    "    if 'gpt2' in model_name or \"bert\" in model_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown model: {model_name}')\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def load_json_dataset(data_path, max_num_data):\n",
    "    texts = []\n",
    "    for i, line in enumerate(open(data_path)):\n",
    "        if i >= max_num_data:\n",
    "            break\n",
    "        texts.append(json.loads(line)['text'])\n",
    "    return texts\n",
    "\n",
    "\n",
    "def load_and_tokenize_json_data(tokenizer, data_path, max_len=1024, max_num_data=float('inf')):\n",
    "    \"\"\" Load and tokenize the data in a jsonl format\n",
    "\n",
    "    :param tokenizer:  HF tokenizer object\n",
    "    :param data_path: jsonl file to read. Read the \"text\" field of each line\n",
    "    :param max_len: maximum length of tokenized data\n",
    "    :param max_num_data: maximum number of lines to load\n",
    "    :return: list of `torch.LongTensor`s of shape (1, num_tokens), one for each input line\n",
    "    \"\"\"\n",
    "    assert max_len <= 1024 and max_num_data >= 2000, f\"max_len={max_len}, max_num_data={max_num_data} are insufficent\"\n",
    "    t1 = time.time()\n",
    "    texts = load_json_dataset(data_path, max_num_data=max_num_data)\n",
    "    t2 = time.time()\n",
    "    print(f'dataset load time: {round(t2 - t1, 2)} sec')\n",
    "    t1 = time.time()\n",
    "    tokenized_texts = [tokenizer.encode(sen, return_tensors='pt', truncation=True, max_length=max_len)\n",
    "                       for sen in texts]\n",
    "    t2 = time.time()\n",
    "    print(f'tokenizing time: {round(t2 - t1, 2)} sec')\n",
    "    return tokenized_texts\n",
    "\n",
    "\n",
    "def decode_samples_from_lst(tokenizer, tokenized_texts):\n",
    "    \"\"\" Decode from tokens to string\n",
    "\n",
    "    :param tokenizer: HF tokenizer\n",
    "    :param tokenized_texts: list of list of tokens\n",
    "    :return: decoded output as a list of strings of the same length as tokenized_text_list\n",
    "    \"\"\"\n",
    "    t1 = time.time()\n",
    "    output = []\n",
    "    for l in tokenized_texts:\n",
    "        o = tokenizer.decode(torch.LongTensor(l), skip_special_tokens=True)\n",
    "        output.append(o)\n",
    "    t2 = time.time()\n",
    "    print(f'de-tokenizing time: {round(t2 - t1, 2)}')\n",
    "    return output\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def featurize_tokens_from_model(model, tokenized_texts, batch_size, name=\"\", verbose=False):\n",
    "    \"\"\"Featurize tokenized texts using models, support batchify\n",
    "    :param model: HF Transformers model\n",
    "    :param batch_size: Batch size used during forward pass\n",
    "    :param tokenized_texts: list of torch.LongTensor of shape (1, length)\n",
    "    :param verbose: If True, print status and time\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    t1 = time.time()\n",
    "    feats, chunks, chunk_sent_lengths = [], [], []\n",
    "    chunk_idx = 0\n",
    "\n",
    "    while chunk_idx * batch_size < len(tokenized_texts):\n",
    "        _chunk = [_t.view(-1) for _t in tokenized_texts[chunk_idx * batch_size: (chunk_idx + 1) * batch_size]]\n",
    "        chunks.append(_chunk)\n",
    "        chunk_sent_lengths.append([len(_c) for _c in _chunk])\n",
    "        chunk_idx += 1\n",
    "\n",
    "    for chunk, chunk_sent_length in tqdm(list(zip(chunks, chunk_sent_lengths)), desc=f\"Featurizing {name}\"):\n",
    "        padded_chunk = torch.nn.utils.rnn.pad_sequence(chunk,\n",
    "                                                       batch_first=True,\n",
    "                                                       padding_value=0).to(device)\n",
    "        attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.ones(sent_length).long() for sent_length in chunk_sent_length],\n",
    "            batch_first=True,\n",
    "            padding_value=0).to(device)\n",
    "        outs = model(input_ids=padded_chunk,\n",
    "                     attention_mask=attention_mask,\n",
    "                     past_key_values=None,\n",
    "                     output_hidden_states=True,\n",
    "                     return_dict=True)\n",
    "        h = []\n",
    "        for hidden_state, sent_length in zip(outs.hidden_states[-1], chunk_sent_length):\n",
    "            h.append(hidden_state[sent_length - 1])\n",
    "        h = torch.stack(h, dim=0)\n",
    "        feats.append(h.cpu())\n",
    "    t2 = time.time()\n",
    "    if verbose:\n",
    "        print(f'Featurize time: {round(t2 - t1, 2)}')\n",
    "    return torch.cat(feats)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:41:19.719722100Z",
     "start_time": "2024-05-05T09:41:19.717721900Z"
    }
   },
   "id": "6d1b83eb77b455c1"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def compute_mauve(\n",
    "        p_features=None, q_features=None,\n",
    "        p_tokens=None, q_tokens=None,\n",
    "        p_text=None, q_text=None,\n",
    "        num_buckets='auto', pca_max_data=-1, kmeans_explained_var=0.9,\n",
    "        kmeans_num_redo=5, kmeans_max_iter=500,\n",
    "        featurize_model_name='gpt2-large', device_id=-1, max_text_length=1024,\n",
    "        divergence_curve_discretization_size=25, mauve_scaling_factor=5,\n",
    "        verbose=False, seed=25, batch_size=1, use_float64=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the MAUVE score between two text generations P and Q.\n",
    "\n",
    "    P is either specified as ``p_features``, ``p_tokens``, or ``p_text``. Same with Q.\n",
    "\n",
    "    :param p_features: ``numpy.ndarray`` of shape (n, d), where n is the number of generations.\n",
    "    :param q_features: ``numpy.ndarray`` of shape (n, d), where n is the number of generations.\n",
    "    :param p_tokens: list of length n, each entry is torch.LongTensor of shape (1, length).\n",
    "    :param q_tokens: list of length n, each entry is torch.LongTensor of shape (1, length).\n",
    "    :param p_text: list of length n, each entry is a string.\n",
    "    :param q_text: list of length n, each entry is a string.\n",
    "    :param num_buckets: the size of the histogram to quantize P and Q. Options: ``'auto'`` (default, which is n/10) or an integer.\n",
    "    :param pca_max_data: the number data points to use for PCA. If `-1`, use all the data. Default -1.\n",
    "    :param kmeans_explained_var: amount of variance of the data to keep in dimensionality reduction by PCA. Default 0.9.\n",
    "    :param kmeans_num_redo: number of times to redo k-means clustering (the best objective is kept). Default 5.\n",
    "        Try reducing this to 1 in order to reduce running time.\n",
    "    :param kmeans_max_iter: maximum number of k-means iterations. Default 500.\n",
    "        Try reducing this to 100 in order to reduce running time.\n",
    "    :param featurize_model_name: name of the model from which features are obtained. Default 'gpt2-large'.\n",
    "        We support all models which can be loaded from ``transformers.AutoModel.from_pretrained(featurize_model_name)``.\n",
    "    :param device_id: Device for featurization. Supply gpu_id (e.g. 0 or 3) to use GPU or -1 to use CPU.\n",
    "    :param max_text_length: maximum number of tokens to consider. Default 1024.\n",
    "    :param divergence_curve_discretization_size: Number of points to consider on the divergence curve. Default 25.\n",
    "        Larger values do not offer much of a difference.\n",
    "    :param mauve_scaling_factor: The constant``c`` from the paper. Default 5.\n",
    "        See `Best Practices <index.html#best-practices-for-mauve>`_ for details.\n",
    "    :param verbose: If True, print running time updates.\n",
    "    :param seed: random seed to initialize k-means cluster assignments.\n",
    "    :param batch_size: Batch size for feature extraction.\n",
    "        A larger batch size speeds up computation.\n",
    "        You might have to experiment to find the largest batch size that fits in your GPU memory.\n",
    "        See `here <https://github.com/krishnap25/mauve/issues/8#issuecomment-1082075240>`_ for details.\n",
    "\n",
    "    :return: an object with fields p_hist, q_hist, divergence_curve and mauve.\n",
    "\n",
    "    * ``out.mauve`` is a number between 0 and 1, the MAUVE score. Higher values means P is closer to Q.\n",
    "    * ``out.frontier_integral``, a number between 0 and 1. Lower values mean that P is closer to Q.\n",
    "    * ``out.p_hist`` is the obtained histogram for P. Same for ``out.q_hist``.\n",
    "    * ``out.divergence_curve`` contains the points in the divergence curve. It is of shape (m, 2), where m is ``divergence_curve_discretization_size``\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if p_features is None and p_tokens is None and p_text is None:\n",
    "        raise ValueError('Supply at least one of p_features, p_tokens, p_text')\n",
    "    if q_features is None and q_tokens is None and q_text is None:\n",
    "        raise ValueError('Supply at least one of q_features, q_tokens, q_text')\n",
    "    p_features = get_features_from_input(\n",
    "        p_features, p_tokens, p_text, featurize_model_name, max_text_length,\n",
    "        device_id, name=\"p\", verbose=verbose, batch_size=batch_size, use_float64=use_float64,\n",
    "    )\n",
    "    q_features = get_features_from_input(\n",
    "        q_features, q_tokens, q_text, featurize_model_name, max_text_length,\n",
    "        device_id, name=\"q\", verbose=verbose, batch_size=batch_size, use_float64=use_float64,\n",
    "    )\n",
    "    if num_buckets == 'auto':\n",
    "        # heuristic: use num_clusters = num_generations / 10\n",
    "        num_buckets = max(2, int(round(min(p_features.shape[0], q_features.shape[0]) / 10)))\n",
    "    elif not isinstance(num_buckets, int):\n",
    "        raise ValueError('num_buckets is expected to be an integer or \"auto\"')\n",
    "\n",
    "    # Acutal binning\n",
    "    t1 = time.time()\n",
    "    p, q = cluster_feats(p_features, q_features,\n",
    "                         num_clusters=num_buckets,\n",
    "                         norm='l2', whiten=False,\n",
    "                         pca_max_data=pca_max_data,\n",
    "                         explained_variance=kmeans_explained_var,\n",
    "                         num_redo=kmeans_num_redo,\n",
    "                         max_iter=kmeans_max_iter,\n",
    "                         seed=seed, verbose=verbose)\n",
    "    t2 = time.time()\n",
    "    if verbose:\n",
    "        print('total discretization time:', round(t2 - t1, 2), 'seconds')\n",
    "\n",
    "    # Divergence curve and mauve\n",
    "    mixture_weights = np.linspace(1e-6, 1 - 1e-6, divergence_curve_discretization_size)\n",
    "    divergence_curve = get_divergence_curve_for_multinomials(p, q, mixture_weights, mauve_scaling_factor)\n",
    "    x, y = divergence_curve.T\n",
    "    idxs1 = np.argsort(x)\n",
    "    idxs2 = np.argsort(y)\n",
    "    mauve_score = 0.5 * (\n",
    "            compute_area_under_curve(x[idxs1], y[idxs1]) +\n",
    "            compute_area_under_curve(y[idxs2], x[idxs2])\n",
    "    )\n",
    "    fi_score = get_frontier_integral(p, q)\n",
    "    to_return = SimpleNamespace(\n",
    "        p_hist=p, q_hist=q, divergence_curve=divergence_curve,\n",
    "        mauve=mauve_score,\n",
    "        frontier_integral=fi_score,\n",
    "        num_buckets=num_buckets,\n",
    "    )\n",
    "    return to_return\n",
    "\n",
    "\n",
    "def get_features_from_input(features, tokenized_texts, texts,\n",
    "                            featurize_model_name, max_len, device_id, name, batch_size,\n",
    "                            verbose=False, use_float64=False):\n",
    "    global MODEL, TOKENIZER, MODEL_NAME\n",
    "    if features is None:\n",
    "        # Featurizing is necessary. Make sure the required packages are available\n",
    "        if not FOUND_TORCH:\n",
    "            raise ModuleNotFoundError(\n",
    "                \"\"\"PyTorch not found. Please install PyTorch if you would like to use the featurization.\n",
    "                    For details, see `https://github.com/krishnap25/mauve` \n",
    "                    and `https://pytorch.org/get-started/locally/`.\n",
    "                \"\"\")\n",
    "        if not FOUND_TRANSFORMERS:\n",
    "            raise ModuleNotFoundError(\n",
    "                \"\"\"Transformers not found. Please install Transformers if you would like to use the featurization.\n",
    "                    For details, see `https://github.com/krishnap25/mauve` \n",
    "                    and `https://huggingface.co/transformers/installation.html`.\n",
    "                \"\"\")\n",
    "\n",
    "        if tokenized_texts is None:\n",
    "            # tokenize texts\n",
    "            if TOKENIZER is None or MODEL_NAME != featurize_model_name:\n",
    "                if verbose: print('Loading tokenizer')\n",
    "                TOKENIZER = get_tokenizer(featurize_model_name)\n",
    "            if verbose: print('Tokenizing text...')\n",
    "            tokenized_texts = [\n",
    "                TOKENIZER.encode(sen, return_tensors='pt', truncation=True, max_length=max_len)\n",
    "                for sen in texts\n",
    "            ]\n",
    "        # use tokenized_texts to featurize\n",
    "        if TOKENIZER is None or MODEL_NAME != featurize_model_name:\n",
    "            if verbose: print('Loading tokenizer')\n",
    "            TOKENIZER = get_tokenizer(featurize_model_name)\n",
    "        if MODEL is None or MODEL_NAME != featurize_model_name:\n",
    "            if verbose: print('Loading model')\n",
    "            MODEL = get_model(featurize_model_name, TOKENIZER, device_id)\n",
    "            MODEL_NAME = featurize_model_name\n",
    "        else:\n",
    "            MODEL = MODEL.to(get_device_from_arg(device_id))\n",
    "        if use_float64:\n",
    "            MODEL = MODEL.double()\n",
    "        if verbose: print('Featurizing tokens')\n",
    "        features = featurize_tokens_from_model(MODEL, tokenized_texts, batch_size, name).detach().cpu().numpy()\n",
    "    else:\n",
    "        features = np.asarray(features)\n",
    "    return features\n",
    "\n",
    "\n",
    "def cluster_feats(p, q, num_clusters,\n",
    "                  norm='none', whiten=True,\n",
    "                  pca_max_data=-1,\n",
    "                  explained_variance=0.9,\n",
    "                  num_redo=5, max_iter=500,\n",
    "                  seed=0, verbose=False):\n",
    "    assert 0 < explained_variance < 1\n",
    "    if verbose:\n",
    "        print(f'seed = {seed}')\n",
    "    assert norm in ['none', 'l2', 'l1', None]\n",
    "    data1 = np.vstack([q, p])\n",
    "    if norm in ['l2', 'l1']:\n",
    "        data1 = normalize(data1, norm=norm, axis=1)\n",
    "    pca = PCA(n_components=None, whiten=whiten, random_state=seed + 1)\n",
    "    if pca_max_data < 0 or pca_max_data >= data1.shape[0]:\n",
    "        pca.fit(data1)\n",
    "    elif 0 < pca_max_data < data1.shape[0]:\n",
    "        rng = np.random.RandomState(seed + 5)\n",
    "        idxs = rng.choice(data1.shape[0], size=pca_max_data, replace=False)\n",
    "        pca.fit(data1[idxs])\n",
    "    else:\n",
    "        raise ValueError(f'Invalid argument pca_max_data={pca_max_data} with {data1.shape[0]} datapoints')\n",
    "    s = np.cumsum(pca.explained_variance_ratio_)\n",
    "    idx = np.argmax(s >= explained_variance)  # last index to consider\n",
    "    if verbose:\n",
    "        print(f'performing clustering in lower dimension = {idx}')\n",
    "    data1 = pca.transform(data1)[:, :idx + 1]\n",
    "    # Cluster\n",
    "    data1 = data1.astype(np.float32)\n",
    "    t1 = time.time()\n",
    "    kmeans = faiss.Kmeans(data1.shape[1], num_clusters, niter=max_iter,\n",
    "                          verbose=verbose, nredo=num_redo, update_index=True,\n",
    "                          seed=seed + 2)\n",
    "    kmeans.train(data1)\n",
    "    _, labels = kmeans.index.search(data1, 1)\n",
    "    labels = labels.reshape(-1)\n",
    "    t2 = time.time()\n",
    "    if verbose:\n",
    "        print('kmeans time:', round(t2 - t1, 2), 's')\n",
    "\n",
    "    q_labels = labels[:len(q)]\n",
    "    p_labels = labels[len(q):]\n",
    "\n",
    "    q_bins = np.histogram(q_labels, bins=num_clusters,\n",
    "                          range=[0, num_clusters], density=True)[0]\n",
    "    p_bins = np.histogram(p_labels, bins=num_clusters,\n",
    "                          range=[0, num_clusters], density=True)[0]\n",
    "    return p_bins / p_bins.sum(), q_bins / q_bins.sum()\n",
    "\n",
    "\n",
    "def kl_multinomial(p, q):\n",
    "    assert p.shape == q.shape\n",
    "    if np.logical_and(p != 0, q == 0).any():\n",
    "        return np.inf\n",
    "    else:\n",
    "        idxs = np.logical_and(p != 0, q != 0)\n",
    "        return np.sum(p[idxs] * np.log(p[idxs] / q[idxs]))\n",
    "\n",
    "\n",
    "def get_divergence_curve_for_multinomials(p, q, mixture_weights, scaling_factor):\n",
    "    # TODO: check if extreme points are needed\n",
    "    divergence_curve = [[0, np.inf]]  # extreme point\n",
    "    for w in np.sort(mixture_weights):\n",
    "        r = w * p + (1 - w) * q\n",
    "        divergence_curve.append([kl_multinomial(q, r), kl_multinomial(p, r)])\n",
    "    divergence_curve.append([np.inf, 0])  # other extreme point\n",
    "    return np.exp(-scaling_factor * np.asarray(divergence_curve))\n",
    "\n",
    "\n",
    "def get_frontier_integral(p, q, scaling_factor=2):\n",
    "    total = 0.0\n",
    "    for p1, q1 in zip(p, q):\n",
    "        if p1 == 0 and q1 == 0:\n",
    "            pass\n",
    "        elif p1 == 0:\n",
    "            total += q1 / 4\n",
    "        elif q1 == 0:\n",
    "            total += p1 / 4\n",
    "        elif abs(p1 - q1) > 1e-8:\n",
    "            t1 = p1 + q1\n",
    "            t2 = p1 * q1 * (math.log(p1) - math.log(q1)) / (p1 - q1)\n",
    "            total += 0.25 * t1 - 0.5 * t2\n",
    "        # else: contribution is 0\n",
    "    return total * scaling_factor\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:41:19.761003100Z",
     "start_time": "2024-05-05T09:41:19.717721900Z"
    }
   },
   "id": "f29b372b36ea2249"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'/tmp/ibEIwVjwsP'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:47:28.596613700Z",
     "start_time": "2024-05-05T09:47:28.572580500Z"
    }
   },
   "id": "539e5246415f6605",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data to eval mauve.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_1516758/2819065709.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# _df = pd.read_csv('miti_predicted_reference.csv')\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0m_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'data to eval mauve.csv'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0m_df\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m                 )\n\u001B[0;32m--> 311\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    584\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    585\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 586\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    587\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    588\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    480\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    481\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 482\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    483\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    484\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    809\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    810\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 811\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    812\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    813\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, engine)\u001B[0m\n\u001B[1;32m   1038\u001B[0m             )\n\u001B[1;32m   1039\u001B[0m         \u001B[0;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1040\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1041\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1042\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m         \u001B[0;31m# open handles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001B[0m in \u001B[0;36m_open_handles\u001B[0;34m(self, src, kwds)\u001B[0m\n\u001B[1;32m    220\u001B[0m         \u001B[0mLet\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mreaders\u001B[0m \u001B[0mopen\u001B[0m \u001B[0mIOHandles\u001B[0m \u001B[0mafter\u001B[0m \u001B[0mthey\u001B[0m \u001B[0mare\u001B[0m \u001B[0mdone\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mtheir\u001B[0m \u001B[0mpotential\u001B[0m \u001B[0mraises\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    221\u001B[0m         \"\"\"\n\u001B[0;32m--> 222\u001B[0;31m         self.handles = get_handle(\n\u001B[0m\u001B[1;32m    223\u001B[0m             \u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    224\u001B[0m             \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    700\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoding\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;34m\"b\"\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    701\u001B[0m             \u001B[0;31m# Encoding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 702\u001B[0;31m             handle = open(\n\u001B[0m\u001B[1;32m    703\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    704\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data to eval mauve.csv'"
     ]
    }
   ],
   "source": [
    "# _df = pd.read_csv('miti_predicted_reference.csv')\n",
    "_df = pd.read_csv('data to eval mauve.csv')\n",
    "_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T09:47:15.500183700Z",
     "start_time": "2024-05-05T09:47:15.477902100Z"
    }
   },
   "id": "2455df50381a63f9"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "_groups = _df.groupby(by='therapist_level')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T08:18:38.361116800Z",
     "start_time": "2024-05-05T08:18:38.342610800Z"
    }
   },
   "id": "767f7115672046a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text...\n",
      "Featurizing tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": "Featurizing p:   0%|          | 0/6652 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e21bc37324e3400cb1b9a8e112378401"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_mauve_results = {}\n",
    "\n",
    "for _therapist_level, _group in _groups:\n",
    "    _predictions = _group['predicted'].tolist()\n",
    "    _references = _group['reference'].tolist()\n",
    "\n",
    "    _results = compute_mauve(p_text=_predictions, q_text=_references,\n",
    "                             # device_id=0, max_text_length=1024, # GPT 2\n",
    "                             device_id=0, max_text_length=2048,  # LLAMA 2\n",
    "                             featurize_model_name=MODEL_NAME, verbose=True)\n",
    "\n",
    "    _mauve_results[_therapist_level] = _results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-05T08:18:38.360115800Z"
    }
   },
   "id": "3a132eea0cab3878"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TOKENIZER"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de977ec06b632c98"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODEL"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8e38452b8448129"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Data points:', _df.shape[0])  # Each group (poor, average, expert) has 1/3 of the data points."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e390c409b1b6d344"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODEL_NAME"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14cd543d6eb7a888"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_mauve_results['poor'].mauve"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef1a7ffd4b9763b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_mauve_results['average'].mauve"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45b30339d08e4c3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_mauve_results['expert'].mauve"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33ba967512fd80fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "13afd11613b387eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "81506defa6f668f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5db57edbe4c9dfc2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "GPT2:\n",
    "    Poor - 0.4862774818568964\n",
    "    Average - 0.4970733660733661\n",
    "    Expert - 0.3390308444781106"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2ed7b6107a55fac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
