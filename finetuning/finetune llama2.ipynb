{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following notebook based on:\n",
    "https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/fine-tune-llms-in-2024-with-trl.ipynb\n"
   ],
   "id": "b13e01f7e850633e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Install Pytorch & other libraries\n",
    "# !pip install \"torch==2.1.2\" tensorboard\n",
    "# \n",
    "# # Install Hugging Face libraries\n",
    "# !pip install  --upgrade \\\n",
    "# \"transformers==4.36.2\" \\\n",
    "# \"datasets==2.16.1\" \\\n",
    "# \"accelerate==0.26.1\" \\\n",
    "# \"evaluate==0.4.1\" \\\n",
    "# \"bitsandbytes==0.42.0\" \\\n",
    "#     # \"trl==0.7.10\" # \\\n",
    "# # \"peft==0.7.1\" \\\n",
    "# \n",
    "# # install peft & trl from github\n",
    "# !pip install git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e --upgrade\n",
    "# !pip install git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f --upgrade"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-16T14:03:44.532488400Z",
     "start_time": "2024-02-16T14:03:44.532488400Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !pip install python-dotenv\n",
    "# !pip install wandb\n",
    "# !pip install ipywidgets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T14:03:44.576830600Z",
     "start_time": "2024-02-16T14:03:44.575832500Z"
    }
   },
   "id": "6125cbcf9670366b",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'  # Must be before importing torch."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T17:04:52.503071Z",
     "start_time": "2024-07-19T17:04:52.490070Z"
    }
   },
   "id": "14c697bd1bfdb1cd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\n",
    "\n",
    "# # install flash-attn\n",
    "# !pip install ninja packaging\n",
    "# !MAX_JOBS=4 pip install flash-attn --no-build-isolation"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T14:03:45.896466100Z",
     "start_time": "2024-02-16T14:03:44.575832500Z"
    }
   },
   "id": "62115faa1b83f00b",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from os.path import join as pj\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from peft import LoraConfig, PeftConfig, AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import setup_chat_format\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from huggingface_hub import login as hf_login\n",
    "import wandb\n",
    "from transformers.models.auto.tokenization_auto import PreTrainedTokenizerFast\n",
    "from data_datasets.AlexanderStreet import alexander_street\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T17:04:29.602486Z",
     "start_time": "2024-07-19T17:04:26.408673Z"
    }
   },
   "id": "a19c4da7a00a0ea7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stav3\\anaconda3\\envs\\ReichmanThesis\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "load_dotenv(Path('../.env'))",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T17:04:36.100945Z",
     "start_time": "2024-07-19T17:04:36.094431Z"
    }
   },
   "id": "7c35c8898d313cc0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/stavyo/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "hf_login(token=os.getenv('HF_TOKEN'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T14:03:47.621340800Z",
     "start_time": "2024-02-16T14:03:47.451370500Z"
    }
   },
   "id": "f52484d3b1384020",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mstav-dev95\u001B[0m (\u001B[33mstav_nlp\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /home/stavyo/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=os.getenv('WANDB_TOKEN'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T14:03:49.386971300Z",
     "start_time": "2024-02-16T14:03:47.660472100Z"
    }
   },
   "id": "ed3a6ae97e503453",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# noinspection SpellCheckingInspection\n",
    "def load_miti() -> Dict[str, List[Tuple[bool, str]]]:\n",
    "    df = pd.read_csv(MITI_FILE_PATH)\n",
    "    df = df[df['Filename'].str.contains('.txt')]\n",
    "\n",
    "    data = {}\n",
    "    filtered = df[['Filename', 'Only Text']].to_dict('records')\n",
    "\n",
    "    fix_text = lambda x: x.replace('T:', '[THERAPIST]').replace('C:', '[PATIENT]').strip()\n",
    "\n",
    "    timecode_pattern = r\"\\[?\\(?(inaudible )?(at )?\\(?[0-9]{1,2}:[0-9]{1,2}:[0-9]{1,2}\\)?\\]?\"\n",
    "    spaces_pattern = r\" +\"\n",
    "    tabs_pattern = r\"\\t\"\n",
    "    clean_text_timecode = lambda x: re.sub(timecode_pattern, \" \", x)\n",
    "    clean_text_tabs = lambda x: re.sub(tabs_pattern, \" \", x)\n",
    "    clean_text_spaces = lambda x: re.sub(spaces_pattern, \" \", x)\n",
    "    clean_text = lambda x: clean_text_spaces(clean_text_tabs(clean_text_timecode(x)))\n",
    "\n",
    "    fix_and_clean = lambda x: clean_text(fix_text(x))\n",
    "\n",
    "    for row in filtered:\n",
    "        filename = row['Filename']\n",
    "        if filename not in data:\n",
    "            conv = list(map(fix_and_clean, row['Only Text'].split('\\n')))  # Fix text and split by new line\n",
    "\n",
    "            for i in range(len(conv) - 1, 0, -1):\n",
    "                # If the first 9 characters are the same, then it's a continuation of the previous line. len([THERAPIST]) != len([PATIENT])\n",
    "                if conv[i][:9] == conv[i - 1][:9]:\n",
    "                    txt = conv[i].replace('[THERAPIST]', '').replace('[PATIENT]', '').strip()\n",
    "                    conv[i - 1] = conv[i - 1].strip() + ' ' + txt\n",
    "                    conv.pop(i)\n",
    "\n",
    "            # Convert conversation to tuples, where the first element is the role and the second is the text\n",
    "            conv_tup = []\n",
    "            for i in range(len(conv)):\n",
    "                if '[THERAPIST]' in conv[i]:\n",
    "                    conv_tup.append((True, conv[i].replace('[THERAPIST]', '').strip()))\n",
    "                elif '[PATIENT]' in conv[i]:\n",
    "                    conv_tup.append((False, conv[i].replace('[PATIENT]', '').strip()))\n",
    "                else:\n",
    "                    assert False, 'Unknown role'\n",
    "\n",
    "            data[filename] = conv_tup\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_alexander_street() -> Dict[str, List[Tuple[bool, str]]]:\n",
    "    as_dataset = alexander_street.AlexanderStreetDataset.load_dataset(json_file=ALEXANDER_STREET_FILE_PATH)\n",
    "    conversations = as_dataset.volume_ctrn.conversations + as_dataset.volume_psyc.conversations\n",
    "\n",
    "    k = 0\n",
    "    data = {}\n",
    "    for conv in conversations:\n",
    "        utterances = conv.utterances\n",
    "        conv_tup = []\n",
    "        for i in range(len(utterances)):\n",
    "            if 'Therapist:' in utterances[i]:\n",
    "                conv_tup.append((True, utterances[i].replace('Therapist:', '').strip()))\n",
    "            elif 'Patient:' in utterances[i]:\n",
    "                conv_tup.append((False, utterances[i].replace('Patient:', '').strip()))\n",
    "            else:\n",
    "                assert False, f'Unknown role in {utterances[i]}'\n",
    "\n",
    "        data[str(k)] = conv_tup\n",
    "        k += 1\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_prompts(data: List[List[str]], system_message: str, utterances_count: int,\n",
    "                   tokenizer: PreTrainedTokenizerFast,\n",
    "                   max_prompt_length: int) -> List[Dict[List[Dict[str, str]], str]]:\n",
    "    assert utterances_count % 2 == 0, 'utterances_count must be even'\n",
    "\n",
    "    prompts = []\n",
    "    for conversation in tqdm(data):\n",
    "        queue = []\n",
    "        for utt in conversation:\n",
    "            queue.append(utt)\n",
    "\n",
    "            if len(queue) >= utterances_count and queue[-1][0]:\n",
    "                while len(queue) > utterances_count:\n",
    "                    queue.pop(0)\n",
    "\n",
    "                messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "                messages += [{\"role\": 'assistant' if queue[i][0] else \"user\", \"content\": queue[i][1]}\n",
    "                             for i in range(len(queue))]\n",
    "\n",
    "                # noinspection PyBroadException\n",
    "                try:\n",
    "                    tokens = tokenizer.apply_chat_template(messages, tokenize=True)\n",
    "                    if len(tokens) <= max_prompt_length:\n",
    "                        prompts.append({\"messages\": messages})\n",
    "                except:\n",
    "                    print(messages)\n",
    "                    continue\n",
    "\n",
    "                queue.pop(0)\n",
    "\n",
    "    return prompts\n",
    "\n",
    "\n",
    "def create_dataset(train_dataset_size: int, prompts: List[dict],\n",
    "                   train_json_path: str, test_json_path: str):\n",
    "    test_size = abs(train_dataset_size - len(prompts)) / len(prompts)\n",
    "\n",
    "    dataset = Dataset.from_list(prompts)\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    dataset = dataset.train_test_split(test_size=test_size, seed=42)\n",
    "\n",
    "    dataset[\"train\"].to_json(train_json_path, orient=\"records\")\n",
    "    dataset[\"test\"].to_json(test_json_path, orient=\"records\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T14:03:49.411970300Z",
     "start_time": "2024-02-16T14:03:49.386971300Z"
    }
   },
   "id": "a4f206b4dd732e60",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Output directory finetuning/miti/meta-llama-Llama-2-13b-chat-hf-6 already exists",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 43\u001B[0m\n\u001B[1;32m     35\u001B[0m ALEXANDER_STREET_FILE_PATH \u001B[38;5;241m=\u001B[39m pj(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdataset\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124malexander_street_dataset.json\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     37\u001B[0m SYSTEM_MESSAGE \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mYou are a motivational interviewing counselor. \u001B[39m\u001B[38;5;124m'\u001B[39m \\\n\u001B[1;32m     38\u001B[0m                  \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mYou partner with the patient to understand his problems. \u001B[39m\u001B[38;5;124m'\u001B[39m \\\n\u001B[1;32m     39\u001B[0m                  \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mYou are empathetic towards him and help the patient \u001B[39m\u001B[38;5;124m'\u001B[39m \\\n\u001B[1;32m     40\u001B[0m                  \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexplore their ambivalence regarding behavioral change. \u001B[39m\u001B[38;5;124m'\u001B[39m \\\n\u001B[1;32m     41\u001B[0m                  \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mYou are non-judgmental while encouraging the patient to change\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m---> 43\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(OUTPUT_DIR), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOutput directory \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mOUTPUT_DIR\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m already exists\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "\u001B[0;31mAssertionError\u001B[0m: Output directory finetuning/miti/meta-llama-Llama-2-13b-chat-hf-6 already exists"
     ]
    }
   ],
   "source": [
    "UTTERANCES_COUNT = 6\n",
    "MODEL_IDS = {\n",
    "    'Llama-2-7b-chat-hf': 'meta-llama/Llama-2-7b-chat-hf',\n",
    "    'Llama-2-13b-chat-hf': 'meta-llama/Llama-2-13b-chat-hf'\n",
    "}\n",
    "MODEL_ID = MODEL_IDS['Llama-2-13b-chat-hf']\n",
    "TRAINING_FOLDER = 'miti'\n",
    "# wandb.init(name=f'{TRAINING_FOLDER} - {MODEL_ID}')\n",
    "FOLDER_FINETUNING = 'finetuning'\n",
    "OUTPUT_DIR = pj(FOLDER_FINETUNING, TRAINING_FOLDER, f'{MODEL_ID.replace(\"/\", \"-\")}-{UTTERANCES_COUNT}')\n",
    "\n",
    "DATASETS_PATHS = {\n",
    "    'miti': {\n",
    "        'train': pj(FOLDER_FINETUNING, f'prompts_miti_{UTTERANCES_COUNT}_train.json'),\n",
    "        'test': pj(FOLDER_FINETUNING, f'prompts_miti_{UTTERANCES_COUNT}_test.json')\n",
    "    },\n",
    "    'alexander_street_small': {\n",
    "        'train': pj(FOLDER_FINETUNING, f'prompts_alexander_{UTTERANCES_COUNT}_train_small.json'),\n",
    "        'test': pj(FOLDER_FINETUNING, f'prompts_alexander_{UTTERANCES_COUNT}_test_small.json')\n",
    "    },\n",
    "    'alexander_street_large': {\n",
    "        'train': pj(FOLDER_FINETUNING, f'prompts_alexander_{UTTERANCES_COUNT}_train_large.json'),\n",
    "        'test': pj(FOLDER_FINETUNING, f'prompts_alexander_{UTTERANCES_COUNT}_test_large.json')\n",
    "    },\n",
    "    'miti_alexander_street': {\n",
    "        'train': pj(FOLDER_FINETUNING, f'prompts_miti_alexander_{UTTERANCES_COUNT}_train.json'),\n",
    "        'test': pj(FOLDER_FINETUNING, f'prompts_miti_alexander_{UTTERANCES_COUNT}_test.json')\n",
    "    }\n",
    "}\n",
    "\n",
    "TRAIN_DATASET_PATH = DATASETS_PATHS[TRAINING_FOLDER]['train']\n",
    "TEST_DATASET_PATH = DATASETS_PATHS[TRAINING_FOLDER]['test']\n",
    "\n",
    "MITI_FILE_PATH = pj('data_datasets', 'MITI', 'dataset', 'global_mitis.csv')\n",
    "ALEXANDER_STREET_FILE_PATH = pj('dataset', 'alexander_street_dataset.json')\n",
    "\n",
    "SYSTEM_MESSAGE = 'You are a motivational interviewing counselor. ' \\\n",
    "                 'You partner with the patient to understand his problems. ' \\\n",
    "                 'You are empathetic towards him and help the patient ' \\\n",
    "                 'explore their ambivalence regarding behavioral change. ' \\\n",
    "                 'You are non-judgmental while encouraging the patient to change'\n",
    "\n",
    "assert not os.path.exists(OUTPUT_DIR), f'Output directory {OUTPUT_DIR} already exists'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T14:03:49.805395300Z",
     "start_time": "2024-02-16T14:03:49.415970700Z"
    }
   },
   "id": "4be45ff7a9ea4b2f",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "_tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "_tokenizer.padding_side = 'right'  # to prevent warnings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T14:03:49.807395900Z",
     "start_time": "2024-02-16T14:03:49.806441Z"
    }
   },
   "id": "e9cf91786b8d85b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2012c468be56465aa23359ddb8931251"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL = AutoModelForCausalLM.from_pretrained(\n",
    "    'meta-llama/Llama-2-7b-chat-hf',\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "OUTPUT_DIR = MODEL_ID\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "MODEL, TOKENIZER = setup_chat_format(model=MODEL, tokenizer=TOKENIZER)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T14:04:34.659359400Z",
     "start_time": "2024-02-16T14:03:56.846479900Z"
    }
   },
   "id": "2e54baa189a9985",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/5999 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29a98afbcbcf4820aa6cb666388ba3fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/6000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d848bf0159944c7383e2b911b0bcec72"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/20000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46dd5a56ace64ceca7bf827d4a810c9d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea85e7f31c0f4b3782e775bfee101439"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _k in DATASETS_PATHS.keys():\n",
    "    _train_dataset = load_dataset('json', data_files=DATASETS_PATHS[_k]['train'], split=\"train\")\n",
    "    _arr = []\n",
    "\n",
    "    # Iterate all the training dataset\n",
    "    for _idx in tqdm(range(len(_train_dataset))):\n",
    "        _prompt = TOKENIZER.apply_chat_template(conversation=_train_dataset[_idx][\"messages\"],\n",
    "                                                tokenize=True,\n",
    "                                                add_generation_prompt=True)\n",
    "\n",
    "        _arr.append(len(_prompt))\n",
    "\n",
    "    DATASETS_PATHS[_k]['prompts_length'] = _arr"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T14:06:37.514223400Z",
     "start_time": "2024-02-16T14:06:08.027050900Z"
    }
   },
   "id": "59c3383f994b58f8",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miti:\n",
      "\tPrompts count - 5999\n",
      "\tAverage tokens per prompt - 290.72395399233204\n",
      "\tTotal tokens in dataset - 1.744053 (Millions)\n",
      "\n",
      "alexander_street_small:\n",
      "\tPrompts count - 6000\n",
      "\tAverage tokens per prompt - 323.452\n",
      "\tTotal tokens in dataset - 1.940712 (Millions)\n",
      "\n",
      "alexander_street_large:\n",
      "\tPrompts count - 20000\n",
      "\tAverage tokens per prompt - 323.29435\n",
      "\tTotal tokens in dataset - 6.465887 (Millions)\n",
      "\n",
      "miti_alexander_street:\n",
      "\tPrompts count - 10000\n",
      "\tAverage tokens per prompt - 308.3635\n",
      "\tTotal tokens in dataset - 3.083635 (Millions)\n"
     ]
    }
   ],
   "source": [
    "# For each dataset calculate the average length of the prompts and total sum of the lengths\n",
    "for _k in DATASETS_PATHS.keys():\n",
    "    _arr = DATASETS_PATHS[_k]['prompts_length']\n",
    "    _avg = sum(_arr) / len(_arr)\n",
    "    _sum = sum(_arr)\n",
    "\n",
    "    _prompts_count = len(DATASETS_PATHS[_k]['prompts_length'])\n",
    "    print(f'{_k}:\\n\\tPrompts count - {_prompts_count}\\n\\tAverage tokens per prompt - {_avg}\\n\\tTotal tokens in dataset - {_sum / 10 ** 6} (Millions)\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T14:06:37.515226700Z",
     "start_time": "2024-02-16T14:06:37.514223400Z"
    }
   },
   "id": "d3f9f3ff67bb9f6f",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['messages'],\n    num_rows: 5999\n})"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_train_dataset = load_dataset('json', data_files=TRAIN_DATASET_PATH, split=\"train\")\n",
    "_train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T14:06:38.154188800Z",
     "start_time": "2024-02-16T14:06:37.514223400Z"
    }
   },
   "id": "9d700651994b2c95",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['messages'],\n    num_rows: 196\n})"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_dataset = load_dataset('json', data_files=TEST_DATASET_PATH, split=\"train\")\n",
    "_test_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T14:06:38.798231Z",
     "start_time": "2024-02-16T14:06:38.151189800Z"
    }
   },
   "id": "94d6a2b1651d9637",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'messages': [{'content': 'You are a motivational interviewing counselor. You partner with the patient to understand his problems. You are empathetic towards him and help the patient explore their ambivalence regarding behavioral change. You are non-judgmental while encouraging the patient to change',\n   'role': 'system'},\n  {'content': 'Um hmm.', 'role': 'user'},\n  {'content': \"And so when you switch to this pattern I'm suggesting that maybe you pick two or three days and you just do breakfast meals for dinner... not only are breakfast meals usually pretty easily digested, not too heavy, but they are typically mid-range moderate.\",\n   'role': 'assistant'},\n  {'content': 'Um hmm.', 'role': 'user'},\n  {'content': 'If you notice your anxiety go up, get the pen and paper out.',\n   'role': 'assistant'},\n  {'content': 'Ok.', 'role': 'user'},\n  {'content': \"And really try to give a voice to that anxiety. Try to see, if it's one of the inner younger kids that are starting to talk but try to just get some stuff down in black and white.\",\n   'role': 'assistant'}]}"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_train_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T14:06:38.843234Z",
     "start_time": "2024-02-16T14:06:38.797230300Z"
    }
   },
   "id": "1a58158c998f3621",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'messages': [{'content': 'You are a motivational interviewing counselor. You partner with the patient to understand his problems. You are empathetic towards him and help the patient explore their ambivalence regarding behavioral change. You are non-judgmental while encouraging the patient to change',\n   'role': 'system'},\n  {'content': \"Yeah, cause then they smile, and they laugh and then they're happy and that's good.\",\n   'role': 'user'},\n  {'content': 'Yeah.', 'role': 'assistant'},\n  {'content': \"Because then they're not mad at me\", 'role': 'user'},\n  {'content': 'So, in life in general and maybe with your friends and with your family, being able to do things that make people happy is very important. Yeah. And your friends. So, you love to play games with them and hang out.',\n   'role': 'assistant'},\n  {'content': \"Oh, we worked together too but they don't work there either Now, I mean all of us had to leave and so I mean We don't get to see each other like we used to, and it's lonely, I guess.\",\n   'role': 'user'},\n  {'content': \"So, it's been difficult when your friends are not around as much as they used to be.\",\n   'role': 'assistant'}]}"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_dataset[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T14:06:38.844235Z",
     "start_time": "2024-02-16T14:06:38.841234800Z"
    }
   },
   "id": "106066d135848690",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im\\_start|>system\n",
      "\\\\You are a motivational interviewing counselor. You partner with the patient to understand his problems. You are empathetic towards him and help the patient explore their ambivalence regarding behavioral change. You are non-judgmental while encouraging the patient to change<|im\\_end|>\n",
      "\\\\<|im\\_start|>user\n",
      "\\\\I would say maybe a seven<|im\\_end|>\n",
      "\\\\<|im\\_start|>assistant\n",
      "\\\\that's surprising, it's not a one or two, but it's a seven you say it's very good. Why would you say it's not a lower number?<|im\\_end|>\n",
      "\\\\<|im\\_start|>user\n",
      "\\\\Well, I don't want her you know, I don't want to see her struggling with this and maybe me being a drinker around her could potentially influence her I want her to be smart and want to be that role model for her you know<|im\\_end|>\n",
      "\\\\<|im\\_start|>assistant\n",
      "\\\\I understand. what would you say needs what would you say needs to happen to make it a nine for you, instead of a seven for you to really want to go<|im\\_end|>\n",
      "\\\\<|im\\_start|>user\n",
      "\\\\If all that can happen, I would want to change not only for me but for my daughter as well.<|im\\_end|>\n",
      "\\\\<|im\\_start|>assistant\n",
      "\\\\Okay, I, really like that you're always going back to your daughter and you're, always worried about her as well it shows that you're really taking responsibility so if you would like me to assist you in helping you find a session, that's maybe nearby and works with your schedule as you stated, that is your main struggle<|im\\_end|>\n",
      "\\\\\n"
     ]
    }
   ],
   "source": [
    "print(TOKENIZER.apply_chat_template(conversation=_train_dataset[3]['messages'],\n",
    "                                                tokenize=False,\n",
    "                                                add_generation_prompt=False).replace('\\n', '\\n\\\\\\\\').replace('_','\\\\_'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T14:06:38.844235Z",
     "start_time": "2024-02-16T14:06:38.842234100Z"
    }
   },
   "id": "cda38c89fca34a94",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# BitsAndBytesConfig int-4 config\n",
    "_bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "_peft_config = LoraConfig(\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.05,\n",
    "    r=256,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "_training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,  # directory to save and repository id\n",
    "    num_train_epochs=2,  # number of training epochs\n",
    "    per_device_train_batch_size=3,  # batch size per device during training\n",
    "    gradient_accumulation_steps=2,  # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,  # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",  # use fused adamw optimizer\n",
    "    logging_steps=1,  # log every one step\n",
    "    save_strategy=\"epoch\",  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,  # learning rate, based on QLoRA paper\n",
    "    bf16=True,  # use bfloat16 precision\n",
    "    tf32=True,  # use tf32 precision\n",
    "    max_grad_norm=0.3,  # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,  # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",  # use constant learning rate scheduler\n",
    "    push_to_hub=False,  # push model to hub\n",
    "    report_to=[\"wandb\", \"tensorboard\"],  # report metrics to W&B and, tensorboard\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3223b3f97d55a676",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map='auto',\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=_bnb_config\n",
    ")\n",
    "\n",
    "# # set chat template to OAI chatML, remove if you start from a fine-tuned model\n",
    "_model, _tokenizer = setup_chat_format(model=_model, tokenizer=_tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "108609aa58c238f0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "_max_seq_length = 3072  # max sequence length for model and packing of the dataset\n",
    "\n",
    "_trainer = SFTTrainer(\n",
    "    model=_model,\n",
    "    args=_training_args,\n",
    "    train_dataset=_train_dataset,\n",
    "    peft_config=_peft_config,\n",
    "    max_seq_length=_max_seq_length,\n",
    "    tokenizer=_tokenizer,\n",
    "    packing=True,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # We template with special tokens\n",
    "        \"append_concat_token\": False,  # No need to add additional separator token\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "493b052d591439ea",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# start training, the model will be automatically saved to the output directory\n",
    "_trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91789b5b78aecc8e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# save model \n",
    "_trainer.save_model()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91a5002a669a7bc0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # free the memory again\n",
    "del _model\n",
    "del _trainer\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5159f9b2f9260066",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load PEFT model on CPU\n",
    "_config = PeftConfig.from_pretrained(_training_args.output_dir)\n",
    "_model = AutoModelForCausalLM.from_pretrained(_config.base_model_name_or_path, low_cpu_mem_usage=True)\n",
    "_tokenizer = AutoTokenizer.from_pretrained(_training_args.output_dir)\n",
    "_model.resize_token_embeddings(len(_tokenizer))\n",
    "_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    _training_args.output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# # Merge LoRA and base model and save\n",
    "_merged_model = _model.merge_and_unload()\n",
    "_merged_model.save_pretrained(_training_args.output_dir, safe_serialization=True, max_shard_size=\"2GB\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "680ec71c991bc3c7",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
